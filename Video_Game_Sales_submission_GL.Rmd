---
title: "Video Game Sales Submission"
author: "GL"
date: "3/2/2020"
output: pdf_document
---
******
# Introduction

```{r Staging_Code, warning = FALSE, echo=FALSE, comment=NA, message = FALSE}
# Video Games Sales prediction model
# The original data set has many different variables for Sales: "Global","NA","EU","JP","Other"
# Since the Global sales relies on the other region sub-categories, prediction will be for NA Sales levels

# Setting up Function to calculate RMSE
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

#########################################################
# Download Dataset and create training and testing sets #
#########################################################

# Prepare Packages and install them if they are not installed on the user's machine
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(Amelia)) install.packages("Amelia", repos = "http://cran.us.r-project.org")


# Data set originally downloaded from this website and uploaded to GitHub as part of capstone
# https://www.kaggle.com/annavictoria/ml-friendly-public-datasets?utm_medium=email&utm_source=intercom&utm_campaign=data+projects+onboarding
# https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings

data_url <- "https://raw.githubusercontent.com/qquu50/Capstone/master/Video_Games_Sales.csv"


# Download the Video Game Sales into a staging table
# It will then be modified to induce the Scores, Counts, and Year of Release into numerics
# NAs will be introduced by coercion because many games are missing User/Critic scores
raw_vg_table <- read.csv(data_url, stringsAsFactors = FALSE) %>% filter(!is.na(Name))

# Add NA Sales Ranking to table
# This ranking will just be to see the count of games for each bucket
# All sales figures are "millions of units"
raw_vg_table <- raw_vg_table %>% mutate(Sales_Rank = ifelse(NA_Sales >= 1, 6,
                                                            ifelse(NA_Sales >= 0.5, 5, 
                                                                   ifelse(NA_Sales >= 0.25, 4, 
                                                                          ifelse(NA_Sales >= 0.1, 3, 
                                                                                 ifelse(NA_Sales >= 0.05, 2, 1))))))
rank_names <- c("Below 50,000","50,000 - 100,000","100,000 - 250,000","250,000 - 500,000","500,000 - 1 million","Above 1 million units sold")
ranking_list <- raw_vg_table %>% group_by(Sales_Rank) %>% summarize(n = n()) %>% as.data.frame()
rownames(ranking_list) <- rank_names
ranking_list <- ranking_list[order(-ranking_list$Sales_Rank),]
```

The following report will describe the predictive data model created for Video Game Sales using the data provided from Kaggle.com using the URL below. The data is a web scrape of Metacritic ratings and information for video games. There are several columns detailing the level of unit sales, split by each major video game buying region: North America, Europe and Japan, as well as the Genre, Publisher, Platform and other fields. The list below shows all the columns included in the web scraped dataset.\newline
Data URL: https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings
 
``` {r, echo=FALSE, warning = FALSE, comment=NA, message = FALSE}
print("Column Names")
print(colnames(raw_vg_table))
```
 
See below for an example of the data set and its contents:
```{r, echo=FALSE, warning = FALSE, comment=NA, message = FALSE}
raw_vg_table %>% filter(!is.na(User_Score)) %>% head()
```
 
Since the sales figures from each region will inevitably feed into the Global sales, this predictive model will be looking at NA sales as a final predicted value.

The following table is a grouping of ranks the games by the number of units sold. The top 10 sellers in the data set are shown below.
```{r, echo=FALSE, warning = FALSE, comment=NA, message = FALSE}
print(ranking_list)
print(raw_vg_table %>% select(Name, Platform, Year_of_Release ,NA_Sales)%>% top_n(10, NA_Sales))
```
We can see that the vast majority of the population sold less than 1 million units. The data models will attempt to see which variables tend to influence the level of sales in North America and if predictions can be made to see how many sales a video game will have. 
\newpage 
The table below gives a breakdown of the data set structure to see if we can even use some of the columns, or if we need to clean up the data. We will have to coerce the User and Critic Scores/Counts and Year of Release columns into numerics, which will end up introducing some "NAs" into the list, but it will allow for consistency in the data model.
```{r, echo=FALSE, warning = FALSE, comment=NA, message = FALSE}
summary(raw_vg_table)
```

\newpage 
******
# Methods and Analysis

First we start off with viewing the population of the training set. The black highlighting indicates missing data in the training set. We will still attempt to use the User/Critic Scores in predicting NA Sales.

```{r Coersion, echo=FALSE, warning = FALSE, comment=NA, message = FALSE}
# Process the raw VG sales table into something we can use for analysis
raw_vg_table$Critic_Score <- round(as.numeric(raw_vg_table$Critic_Score), digits = 0)
raw_vg_table$Critic_Count <- as.numeric(raw_vg_table$Critic_Count)
raw_vg_table$User_Score <- round(as.numeric(raw_vg_table$User_Score), digits = 0)
raw_vg_table$User_Count <- as.numeric(raw_vg_table$User_Count)
raw_vg_table$Year_of_Release <- as.numeric(raw_vg_table$Year_of_Release)

# Set up training and validation sets 
# Partition Data
# 80/20 split to generate the train(80) and test(20) set
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = raw_vg_table$NA_Sales, times = 1, p = 0.2, list = FALSE)
train_set <- raw_vg_table[-test_index,]
test_set <- raw_vg_table[test_index,]
```

``` {r Histograms1, echo=FALSE, comment=NA, message = FALSE, fig.width=7, fig.asp = 0.618}
# missmap function maps out the missing values per variable
# In this situation, it's missing many of the User/Critic Scores and Counts so those fields may not be useful in the modelling
missmap(train_set, main = 'Missing Map', col = c('black','white'), legend = FALSE, x.cex = 0.8, margins = c(6,4))
```

 
 
The following graphs show the distribution of the video games by NA Sales. The definition of rankings is provided in the Introduction.

``` {r Histograms2, warning = FALSE, echo=FALSE, comment=NA, message = FALSE, fig.width=5, fig.asp = 0.618}
# Due to the variability in NA Sales, it will be split into several buckets and a prediction will be made based on these
train_set %>% 
  ggplot(aes(NA_Sales)) + 
  geom_histogram(bins = 10) + 
  xlab("NA Sales (millions)") + 
  ylab("Count of Games") +
  ggtitle("Raw NA Sales") +
  theme_economist() + 
  theme(plot.title = element_text(size = 10, face = "bold"))
```


``` {r Histograms3, warning = FALSE, echo=FALSE, comment=NA, message = FALSE, fig.width=5, fig.asp = 0.618}
train_set %>% 
  ggplot(aes(Sales_Rank)) + 
  geom_histogram(binwidth = 1) + 
  xlab("NA Sales Ranking") + 
  ylab("Count of Games") +
  ggtitle("Ranking of NA Sales (Higher number means more sales)") +
  theme_economist() + 
  theme(plot.title = element_text(size = 10, face = "bold"))
```


``` {r Histograms4, warning = FALSE, echo=FALSE, comment=NA, message = FALSE, fig.width=5, fig.asp = 0.618}
train_set %>% 
  filter(NA_Sales <= 0.5) %>% 
  ggplot(aes(NA_Sales)) + 
  geom_histogram(bins = 10) + 
  xlab("NA Sales (millions)") + 
  ylab("Count of Games") +
  ggtitle("Sales below 500,000") + 
  theme_economist() + 
  theme(plot.title = element_text(size = 10, face = "bold"))
```
 
 Next we will view if there is any correlation with the numeric variables.

``` {r Correlations, warning = FALSE, echo=FALSE, comment=NA, message = FALSE, fig.width=7, fig.asp = 0.618}
# Check Correlation between numerics
numeric_cols <- sapply(train_set, is.numeric)
correlation_data <- cor(train_set[,numeric_cols])
corrplot(correlation_data)
```

We can see that the Sales figures will have a clear correlation but User or Critic scores and the number of reviews don't directly impact other factors.
Creating a predictive model will have some level of uncertainty, especially with the large number of video games with very low sales and a comparatively smaller selection of games that had enormous success.

``` {r Data_Models, warning = FALSE, echo=FALSE, comment=NA, message = FALSE}
# Just the Average
# As a baseline, check the predictive capability if we take the average NA Sales and use that to predict the test set's sales
mu_hat <- mean(train_set$NA_Sales)
naive_rmse <- RMSE(test_set$NA_Sales, mu_hat)
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

############################
# Platform Effect Model
# Testing the effects that different video game platforms have on sales in the NA region
rmses <- function(){
  mu <- mean(test_set$NA_Sales)
  platform_avg <- test_set %>%
    group_by(Platform) %>%
    summarize(b_i = mean(NA_Sales - mu))
  predicted_sales <- mu + test_set %>%
    left_join(platform_avg, by = "Platform") %>%
    .$b_i
  return(RMSE(predicted_sales, test_set$NA_Sales))
}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Platform Effect Model",  
                                     RMSE = min(rmses())))

############################
# Platform + Genre Effect Model
# Building on the previous model, check to see if the Genres of games have an impact on sales in NA
rmses <- function(){
  mu <- mean(test_set$NA_Sales)
  platform_avg <- test_set %>%
    group_by(Platform) %>%
    summarize(b_i = mean(NA_Sales - mu))
  genre_avg <- test_set %>%
    left_join(platform_avg, by = 'Platform') %>%
    group_by(Genre) %>%
    summarize(b_g = mean(NA_Sales - mu - b_i))
  predicted_sales <- mu + test_set %>%
    left_join(platform_avg, by = "Platform") %>%
    left_join(genre_avg, by = "Genre") %>%
    mutate(pred = mu + b_i + b_g) %>%
    .$pred
  return(RMSE(predicted_sales, test_set$NA_Sales))
}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Platform + Genre Effect Model",  
                                     RMSE = min(rmses())))

############################
# Regularized Platform Effect Model
# Regularizing the original model that relies only on Platform variable to predict NA Sales
# Check to see if regularizing will improve predictions
# Train to find the best lambda
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$NA_Sales)
  b_i <- train_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  predicted_ratings <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, train_set$NA_Sales))
})

lambda <- lambdas[which.min(rmses)]

# Test prediction capability with the lambda from the previous step
# Using test set for this function
rmses <- sapply(lambda, function(l){
  mu <- mean(test_set$NA_Sales)
  b_i <- test_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$NA_Sales))
})

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Platform Effect Model",  
                                     RMSE = min(rmses)))

############################
# Regularized Platform + Genre Effect Model
# Regularizing the original model that relies on Platform and Genre variables to predict NA Sales
# Check to see if regularizing will improve predictions
# Train to find the best lambda
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$NA_Sales)
  b_i <- train_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  predicted_ratings <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>% 
    mutate(pred = mu + b_i + b_g) %>%
    .$pred
  return(RMSE(predicted_ratings, train_set$NA_Sales))
})

lambda <- lambdas[which.min(rmses)]

# Test prediction capability with the lambda from the previous step
# Using test set for this function
rmses <- sapply(lambda, function(l){
  mu <- mean(test_set$NA_Sales)
  b_i <- test_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    mutate(pred = mu + b_i + b_g) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$NA_Sales))
})

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Platform + Genre Effect Model",  
                                     RMSE = min(rmses)))

############################
# Regularized Platform + Genre + Critic Score + User Score Effect Model
# Expanding model to include Critic and User Scores
# Train to find the best lambda
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$NA_Sales)
  b_i <- train_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  b_c <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    group_by(Critic_Score) %>%
    summarize(b_c = sum(NA_Sales - b_i - b_g - mu)/(n()+l))
  b_u <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    left_join(b_c, by= "Critic_Score") %>%
    group_by(User_Score) %>%
    summarize(b_u = sum(NA_Sales - b_i - b_g - b_c - mu)/(n()+l))
  predicted_ratings <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>% 
    left_join(b_c, by= "Critic_Score") %>%
    left_join(b_u, by= "User_Score") %>%
    mutate(pred = mu + b_i + b_g + b_c + b_u) %>%
    .$pred
  return(RMSE(predicted_ratings, train_set$NA_Sales))
})

lambda <- lambdas[which.min(rmses)]

# Test prediction capability with the lambda from the previous step
# Using test set for this function
rmses <- sapply(lambda, function(l){
  mu <- mean(test_set$NA_Sales)
  b_i <- test_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  b_c <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    group_by(Critic_Score) %>%
    summarize(b_c = sum(NA_Sales - b_i - b_g - mu)/(n()+l))
  b_u <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    left_join(b_c, by= "Critic_Score") %>%
    group_by(User_Score) %>%
    summarize(b_u = sum(NA_Sales - b_i - b_g - b_c - mu)/(n()+l))
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>% 
    left_join(b_c, by= "Critic_Score") %>%
    left_join(b_u, by= "User_Score") %>%
    mutate(pred = mu + b_i + b_g + b_c + b_u) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$NA_Sales))
})

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Platform + Genre + Critic Score + User Score Effect Model",  
                                     RMSE = min(rmses)))

############################
# Regularized Platform + Genre + Critic Score + User Score + Year of Release Effect Model
# Further expansion of previous data model to take Year of Release into account
# Train to find the best lambda
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$NA_Sales)
  b_i <- train_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  b_c <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    group_by(Critic_Score) %>%
    summarize(b_c = sum(NA_Sales - b_i - b_g - mu)/(n()+l))
  b_u <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    left_join(b_c, by= "Critic_Score") %>%
    group_by(User_Score) %>%
    summarize(b_u = sum(NA_Sales - b_i - b_g - b_c - mu)/(n()+l))
  b_y <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    left_join(b_c, by= "Critic_Score") %>%
    left_join(b_u, by= "User_Score") %>%
    group_by(Year_of_Release) %>%
    summarize(b_y = sum(NA_Sales - b_i - b_g - b_c - b_u - mu)/(n()+l))
  predicted_ratings <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>% 
    left_join(b_c, by= "Critic_Score") %>%
    left_join(b_u, by= "User_Score") %>%
    left_join(b_y, by="Year_of_Release") %>%
    mutate(pred = mu + b_i + b_g + b_c + b_u + b_y) %>%
    .$pred
  return(RMSE(predicted_ratings, train_set$NA_Sales))
})

lambda <- lambdas[which.min(rmses)]

# Test prediction capability with the lambda from the previous step
# Using test set for this function
rmses <- sapply(lambda, function(l){
  mu <- mean(test_set$NA_Sales)
  b_i <- test_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  b_c <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    group_by(Critic_Score) %>%
    summarize(b_c = sum(NA_Sales - b_i - b_g - mu)/(n()+l))
  b_u <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    left_join(b_c, by= "Critic_Score") %>%
    group_by(User_Score) %>%
    summarize(b_u = sum(NA_Sales - b_i - b_g - b_c - mu)/(n()+l))
  b_y <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    left_join(b_c, by= "Critic_Score") %>%
    left_join(b_u, by= "User_Score") %>%
    group_by(Year_of_Release) %>%
    summarize(b_y = sum(NA_Sales - b_i - b_g - b_c - b_u - mu)/(n()+l))
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>% 
    left_join(b_c, by= "Critic_Score") %>%
    left_join(b_u, by= "User_Score") %>%
    left_join(b_y, by="Year_of_Release") %>%
    mutate(pred = mu + b_i + b_g + b_c + b_u + b_y) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$NA_Sales))
})

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Platform + Genre + Critic Score + User Score + Year of Release Effect Model",  
                                     RMSE = min(rmses)))
```

 
 
Moving on to testing different models, we begin with seeing if just taking the average NA Sales will allow us to closely predict sales of other games
```{r, warning = FALSE, echo=FALSE, comment=NA, message = FALSE}
print(rmse_results[1,])
```
The RMSE of taking just the average is quite high, since the 75th percentile of NA Sales is 0.240. We will try taking different factors into account, specifically, the Platform and Genres to see if they have any impact on the level of sales in North America.

```{r, warning = FALSE, echo=FALSE, comment=NA, message = FALSE}
print(rmse_results[1:3,])
```

An attempt was then made to regularize the data sets and then evaluate predictive power of several options:  
• Regularized Platform Effect Model  
• Regularized Platform + Genre Effect Model  
• Regularized Platform + Genre + Critic Score + User Score Effect Model  
• Regularized Platform + Genre + Critic Score + User Score + Year of Release Effect Model 

Example code for the first regularized model is below:
```{r, warning = FALSE, echo=TRUE, comment=NA, message = FALSE}
############################
# Regularized Platform + Genre Effect Model
# Regularizing the original model that relies on Platform and Genre variables to predict NA Sales
# Check to see if regularizing will improve predictions
# Train to find the best lambda
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$NA_Sales)
  b_i <- train_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  predicted_ratings <- train_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>% 
    mutate(pred = mu + b_i + b_g) %>%
    .$pred
  return(RMSE(predicted_ratings, train_set$NA_Sales))
})

lambda <- lambdas[which.min(rmses)]

# Test prediction capability with the lambda from the previous step
# Using test set for this function
rmses <- sapply(lambda, function(l){
  mu <- mean(test_set$NA_Sales)
  b_i <- test_set %>%
    group_by(Platform) %>%
    summarize(b_i = sum(NA_Sales - mu)/(n()+l))
  b_g <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    group_by(Genre) %>%
    summarize(b_g = sum(NA_Sales - b_i - mu)/(n()+l))
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "Platform") %>%
    left_join(b_g, by = "Genre") %>%
    mutate(pred = mu + b_i + b_g) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$NA_Sales))
})
```

\newpage 
******
# Results

```{r Final_Results, echo=FALSE, comment=NA}
print("Table of RMSEs")
rmse_results %>% knitr::kable()
```
 
Based on RMSE, the best model is:
``` {r, echo=FALSE, comment=NA}
print(rmse_results$method[which.min(rmse_results$RMSE)])
top_score <- format(min(rmse_results$RMSE), format = "f", digits = 6)
print(paste("It has an RMSE score of:", top_score))
```


# Conclusion

In conclusion, the best data model has a RMSE of ~787,000 North America Sales units. When judging by the range of values (0 - 41.6), that seems very powerful, but unfortunately the NA Sales population of video games is heavily skewed towards the lower value. As we can see, the 75th percentile is still below a quarter of a million units sold.
```{r, echo=FALSE, comment=NA}
summary(raw_vg_table$NA_Sales)
```
With the hit or miss nature of video games in general where they either become very popular and sell extremely well or have low sales, we would have to make sacrifices in one direction or another: either keeping the games that sold extremely well (>75th percentile) or keep only the low selling games (below 75th percentile) to allow for for stronger predictive capability. Using the current method favors the huge outliers and skews the results to a higher predictive unit sale figure.